Regression
__________



	* Simple Linear Regression
	  ________________________

		my_model.fit(X_train, y_train)

		y = F(x)
		y = mx + c
		    m -> slope
		    c -> intercept

		y = B1.X + B0           
		    B1 -> slope (coefficient)
		    B0 -> intercept 

		The whole idea is to find perfect value of B0 and B1
		which is representing the best fit line which can 
		estimate y for any given X.

		* Ordinary Least Square method
		
			Y -> Actual value of y
			Y^ -> Predicted value of y

		  1. Start with any random value of B1 and B0. 
		     You will have some error (e)

		  2. Y = Y^ + e  -----------  i
		     Actual value = Predicted value + error
		     Y^ = B1^.X + B0^ ----------- ii

		  3. RSS (Residual sum of squares)
			= Summation over all observations(e^2)
			=  Summation (Y-Y^)^2
			=  Summation (Y-B1^.X - B0^)^2
		      
		  4. We have to minimize RSS because if we minimize 
		     RSS then we will have the best fit line and the
		     best values of B0 and B1. 
		     To Minimize we use partial derivatives with respect to
		     B0 and B1. Solve for B0 and B1. 

		This value of B0 and B1 will be the most appropriate value
	 	for your data. And you can plot the best fit line using
		these values

		* Gradient Descent Method
 		 ________________________

		B0(new) = B0(old) - alpha*slope(Error) --- Equation

		where alpha -> Learning_rate (Hyperparameter)
			       [0.0001, 0.001, 0.01, 0.1, 0.5, 0.6, 1, 5, 10]
		
		* Step_Size
		* Initialization - HE, XAVIER
		* TimeSteps (Iterative)

		
		B0(new) = B0(old) - alpha*slope1 (Error vs variable)
		B1(new) = B1(old) - alpha*slope2

		Advantages :

		1. Although iterative(slow) but scalable
		    

		Example :
		
		A simple Linear Regression on Advertising.csv with
		"TV" as feature and "Sales" as target. 
	
		Task :

		Perform the same run on "Radio" and "Newspaper"
		individually and record the results



* Multiple Linear Regression
  __________________________
		
		In MLR you have the flexibility to train on multiple input
	or features. Remember in simple linear regression you had only one
	feature and one target and hence you had one coefficient and one 
	intercept but in MLR you will have multiple features and hence
	multiple coefficients. 

	Equation of MLR :

	Y = B0 + B1X1 + B2X2+ B3X3 + ....... BnXn (Equation of hyperplane)


* Polynomial Regression (Non Linear Regression)
  _____________________

	Remember that we assumed in the very initial talk about linear regression that the data should be linearly related. But in reality thats not
always the case. The data may or may not be linearly related. What if the data is non linear in nature ? Obviously the Linear Regression will not give you favourbale results. Hence we will have to employ some other mechanism of building a regression model. Here comes the polynomial regression which is the right solution for a non linear data. How does it work?

	Y = ax^2 + bx + c (order - 2)
	    ax^3 + bx^2 + cx + d (order - 3)

	No of Customers = f(Temp, day_of_month)

			= B0 + B1.Temp^2 + B2.Temp + B3.day^2 + B4.day + B5

	Once you have transformed your features into polynomials we can fit
	a linear regression. 

	
	

Summary
_______

	1. You have a data
	2. If all feature in data is linear,
		if feature dimension == 1:
			go with SLR
		else if feature dimension > 1:
			go with MLR

	   else if any feature in data is non linear :
		go with PLR



Error Metrics :

1. Simple Error : Actual - Predicted
		Problem : Errors may cancel each other out since it is just
		an algebraic sum of errors. 

2. Absolute Error : Absolute (Actual - Predicted)
3. Mean Absolute Error : Mean of Absolute Error 
		Problem : No Problem

4. Squared Error : (Actual - Predicted) * (Actual - Predicted)
5. Mean Squared Error
		Problem : The scale of the error has changed. 
		solution : Go with Root mean Squared Error
		Advantage : Since the error is amplified,
			    it makes the model learn quickly by converging
			    faster on miscalculated samples
		Disadvantage : If you have untreated outliers then the 
				squared error will explode, in such a case
				stick to MAE

6. Sum Squared Error
		Problem : No Problem
		Advantage : Used in OLS method for learning coefficients



7. Root Mean Squared Error : 
		Problem : No Problem
		Advantage : It reports the MSE on the same scale as of
				error

All the above error metrics are good for training or reporting but for interpretaion purpose none of them are helpful. Why ?

	Model 1 : RMSE of 1120
	Model 2 : RMSE of 1020

		You will say Model 2 is better than Model 1. 

		How can you explain the goodness of Model 2 ??
		How much variation of the output variable is your model
		able to explain ?
		Here the R-squared comes to rescue. 


8. R-squared Error
	
	It is also a relative error metric but it also explains the goodness
of your model. If I say that R - squared explains that how much variation of
target variable is captured then I wont be wrong in claiming so. 


	What is the minimum and maximum value of R-squared ?

		- infinity to 1
	
	Range : (-infinity to 0)  :  Worst than mean prediction
	Range : 0 : Exact mean predictions
	Range : (0 - 1)  : Better than mean predictions
	Range : 1 : Exact actual predictions




What degree of polynomial is allowed ?

As a ML engineer our objective is to build a model on train data such that it can do well on test data. When we fitted a 10-degree polynomial we got a curve which is fitting very strictly to every train data. Hence we thought that its the best but generally on an unseen data this will fail. The reason is that we did not allow the curve to learn the pattern rather we forced the curve to memorize the pattern. 

	Example :

	Teacher 1 : Strict Teacher - Any small mistake you are gone
	Teacher 2 : Moderate Teacher - Allows you to make small mistakes and 
					learn from it
	Teache 3 : Over Moderate Teacher : Doesnt correct you at all 

	Example :

	Interview

	Panel 1 : Over sensitive - High Variance
			Any single question you answer incorrect and you
			are gone, plus any single answer correctly they will
			hire you assuming you are the most intelligent person
			on earth

	Panel 2 : Moderately sensitive - Moderate variance and moderate bias
			They will ask you set of questions and then they will
			conclude to some actions based on your answers

	Panel 3 : No sensitivity - High Bias

			Either you are answer correctly or incorrectly they 
			are not at all bothered about it


Task :

	Populate the sheet shared as sheet no 4 in ethe excel sheet 
	Post it on the forum once done as an image

1. Multiple Linear Regression
2. Polynomial Regression
Populate a sheet containing various columns with the metrics on train and test







Regularization
______________

	
	In Simple Linear Regression, we assumed that the data is linearly related to the target and hence can be modelled with linear models such as linear regression. But in some cases we find data is more complex than a straight line. What do we do in such cases? If we force to fit a straight line you would observe that the line will never be a good fit to the distribution and hence you find huge errors or less r-squared value. Surprisingly, we can use linear model by adding a simple step of polyomial feature transformation. Add powers of each feature as new features, then train a linear model on this extended set of features. This is called as polynomial regression. 


The Problem
____________

	If you perform a higher degree polynomial regression you are likely fitting the train data much better than with plain regression. But this high degree polynomial comes with its own cost. The curve may fit the training data very precisely but it may start missing the estimation on test data.
And this scenario is called as overfitting. The model overfits the data


Underfitting :

	When the model is unable to fit to the train data and the train error is huge then we say that the model is underfitting. And this happens due to something called as high bias(ignorance) and low variance(sensitive). 


Overfitting :

	When the model is able to fit the train data perfectly and the train error is very small but the test error is huge which is indicating that the model is unable to predict on unseen data then we say the model has overfitted the data. And this happens due to something called as low bias(ignorant) and high variance(sensitive)


Best Fit :

	So Ideally you see that smart people are not very sensitive and not very ignorant. 
	So similarly smart models should not be having high bias and even low bias. So hence a model with moderate bias and moderate variance is a good model and such a model is called as best fit model and the indication of such a model is through the train error and test error falling in the same range. 


Bias : 

	Being ignorant ( wrong assumption )
	The underfit model is an example of a model having high bias and low variance


Variance : 

	Being sensitive ( you answer one question correctly, he will start thinkin
			you are the most intelligent person on earth,
			and the next question you answer incorrectly, he will start thinking
			you are the person having no knowledge at all )

	The overfit model is an example of a model having high variance. 
	Any change in the data will change the model's estimates. 

Target : Moderate variance and moderate bias



Regularization :
________________

	Regularization is the solution to overfitting. But wait what is the solution to underfitting ? The solution to underfitting is to train more. Or improve the data. 


	Regularization basically modifies the loss function and ensures that overfitting is reduced. 

	If you remember in the gradient descent algorithm we used MSE as the loss function. 

	L = MSE

	But Regularization introduces a term in your loss function. 

	L = MSE + lambda.Summation(beta^2)

	Here beta is nothing but the learnable coefficients of your model.
	Remember what we assumed that we should allow the model to make
	certain small mistakes. 

How is this stopping the overfitting ?

We saw that overfitting happened due to high variance. What is high variance? It is nothing but model's sensitivity to any updates in the training data. Correct? This is causing overfitting. So the solution should be to avoid these updates. The model should learn to ignore. How can this be possible? When we say that the model should not be sensitive we are basically trying to say that the model should not update its parameters (B0 and B1) for any change in data. Correct ? But wait who is making the change ? The gradient descent makes the changes. And the gradient descent does so by the inputs from your loss function. Now if somehow I can regulate the loss function then I can regulate the updates. Hence the regularization term is added to the loss function to regulate it.

	The lambda in the regularization term is to control the strength of 
	regularization. The more the value of lambda the more strict the 
	regularization. What is optimal value ? That you will have to find 
	yourself by trying different values for your data. 


Lets say your linear regression equation is this


	y = B0 + B1.X1 + B2.X2 + B3.X3

1. In Ridge Regression the Loss function will be like this


	LF = MSE + lambda. (B1^2 + B2^2 + B3^2)
	Ridge Regression also reduces the problem of multicollinearity by
	reducing the coefficients of redundant variables. 

	For Example in case of price of a house

	if price of a house = f(No of bedrooms, balcony area, 
				no of floors)
			   = B1.Bedrooms + B2.Balcony + B3.Floors + B4
	LF = MSE + lambda.(B1^2 + B2^2 + B3^2)
	
	What is multicollinearity ?

	The problem of multicollinearity is when two input features are highly correlated with each other. Now this actually causes instability in the coefficients of the two models. 

	y = a.X1 + b.X2 + c.X3 + d

	Now my X1 and X2 are highly correlated, which means I can write
	X1 = l.X2 + m

	y = a.(l.X2 + m) + b.X2 + c.X3 + d
	  = a.l.X2 + a.m + b.X2 + c.X3 + d
 	  = (a.l + b).X2 + c.X3 + (d+a.m)

	And this problem causes overfitting. 

	



2. In Lasso Regression the Loss function will be like this


	LF = MSE + lambda. (abs(B1) + abs(B2)+ abs(B3))

	For Example in case of price of a house

	if price of a house = f(No of bedrooms, balcony area, 
				no of floors)
			   = B1.Bedrooms + B2.Balcony + B3.Floors + B4
	LF = MSE + lambda.(abs(B1) + abs(B2)+ abs(B3))

	This helps in feature selection by dropping the coeefficients to zero for variables which are not responsible for predicting the target variable. 

	Lets say for predicting the price of house if I have features like
no of bedrooms, no of balconies, stock price of tesla then you know that the
stock price is an irrelevant variable hence the model will automatically
push the coefficient of stock price of tesla to zero indicating that its an
irrelevant variable and the remaining will be non zero indicating they are 
relevant. 

	The max_iter defines how many times you want the variables to be updated.
If you have different values of iterations then obviously you will have different values of coefficients. 

This concludes the regression methods.


3. Elastic Net

	Hybrid of Ridge and Lasso

	LF = MSE + lambda1. (B1^2 + B2^2 + B3^2) + lambda2. (abs(B1) + abs(B2)+ abs(B3)) 

	Eg :

	if price of a house = f(No of bedrooms, balcony area, 
				no of floors)
			   = B1.Bedrooms + B2.Balcony + B3.Floors + B4
	LF = MSE + lambda1.(abs(B1) + abs(B2)+ abs(B3)) + lambda2. (B1^2 + B2^2 + B3^2)
	



Classification 
______________

	Your target variable is discrete. 

Logistic Regression
___________________


		
	Example : Chances of getting a job ---> y
		   No of hours you study  ----> x

	Example : Chances of you getting a loan  ----> y
		  Credit history ----> x

	Example : Chances of rainfall ----> y
		  Temperature -----> x

	y = logistic function(x)
	  = sigmoid (X)

	y = 1/(1+e^(-W.T*X))

	Within this formula : 

		y -> Target
		x -> Feature
		W - > Learnable parameter (same as B in Linear Regression)


	From the formula we realize that the function can take any range of input values
	but the output will be always between 0 and 1. Hence we infer the output of 
	the sigmoid function as probabilities.

	We say that what is the prob of a person getting a job given he has 4 years of experience?

	If the prob > 0.5 means he will get a job
	If the prob <=0.5 means he wont get a job

	Now if you observe that the output is between 0 and 1. How many values are between 0 and 1?
	There are unlimited values, hence the name is regression but we use these unlimited
	values and infer a discrete output hence the problem solved is a classification
	problem. 

	Example :	
		chances of getting a job = 1/(1+e^(-W.T*No of hours study))

	You need to find the best W value such that the error is least. 

	You might argue that how is this classfication? Because in the
	output I am getting probabilities which is actually continuous.
	Here is the catch.


	Since the output is continuous we say its a regression algorithm but
	since the inference part is like this


	if predicted prob (y^) >= 0.5 ; then prediction = 1
	   else 		< 0.5 ; then prediction = 0 

	The way we interpret or infer is classification. 


	Thats why the name is regression but the purpose which it solves is
	classification. 



	The Loss function ???
	______________________

		Do we use MSE over here ?
		The answer is we can use but we dont. 
		why ? because the output is in terms of yes or no
		and your prediction is coming as probabilities. 
		So we may need some transformation from probabilities
		to discrete output. Hence we avoid direct use of MSE.

	Log Loss :
	__________
	
	if actual(y) = 1 ; then L = -log(p^)
	   actual(y) = 0; then  L = -log(1-p^)


Agenda : 

1. Classification Metrics
2. Implementation
3. Feature Engineering 
	PCA
	Encoders
4. Tree based methods

Task :

	1. Please find the bigmart_train.csv on the github repo. 
	Implement all the regression models and finetune(grid search) them with the best set of parameters. Log in your observation in an excel file for analysis

	2. Go to projects section you can select any one project of your own
	interest and try to attempt the first few questions which are part of
	EDA 
	














	



	
		 










 


		










		     

		